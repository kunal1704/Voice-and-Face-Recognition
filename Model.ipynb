{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77dc1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: facenet_pytorch in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (2.5.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from facenet_pytorch) (0.16.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from facenet_pytorch) (9.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from facenet_pytorch) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from facenet_pytorch) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from requests->facenet_pytorch) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from requests->facenet_pytorch) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from requests->facenet_pytorch) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from requests->facenet_pytorch) (1.26.14)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\toufeeq sk\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch facenet_pytorch tqdm\n",
    "pip install tensorflow --upgrade\n",
    "pip install facenet_pytorch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644cb3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Toufeeq Sk\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Accuracy: 84.82% (Dynamic Threshold: 1.00)\n",
      "Mean Average Precision (mAP): 1.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "from tqdm import tqdm\n",
    "from types import MethodType\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "\n",
    "### helper function\n",
    "def encode(img):\n",
    " res = resnet(torch.Tensor(img))\n",
    " return res\n",
    "\n",
    "# Custom face detection method\n",
    "def detect_box(self, img, save_path=None):\n",
    "  batch_boxes, batch_probs, batch_points = self.detect(img, landmarks=True)\n",
    " \n",
    "  # Select Faces (Skipping small ones)\n",
    "  if not self.keep_all:\n",
    "    small_face_indices = [i for i, box in enumerate(batch_boxes) if box[2] - box[0] < self.min_face_size]\n",
    "    batch_boxes = np.delete(batch_boxes, small_face_indices, axis=0)\n",
    "    batch_probs = np.delete(batch_probs, small_face_indices, axis=0)\n",
    "    if len(batch_points) > 0:\n",
    "      batch_points = np.delete(batch_points, small_face_indices, axis=0)\n",
    "\n",
    "# Extract faces\n",
    "  faces = self.extract(img, batch_boxes, save_path)\n",
    "  return batch_boxes, faces\n",
    "\n",
    "\n",
    "### load model\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "mtcnn = MTCNN(margin=44,image_size=182, keep_all=True, thresholds=[0.4, 0.5, 0.5], min_face_size=60)\n",
    "mtcnn.detect_box = MethodType(detect_box, mtcnn)\n",
    "\n",
    "# Initialize a list to store distances for dynamic thresholding\n",
    "known_distances = []\n",
    "\n",
    "### load known faces\n",
    "saved_pictures = r\"File-path\"\n",
    "all_people_faces = {}\n",
    "\n",
    "# Define detection frequency \n",
    "detection_counter = 0\n",
    "detection_frequency = 5  # Adjust this value for your desired balance between accuracy and speed\n",
    "\n",
    "# Iterate through folders containing individual's pictures\n",
    "for person_folder in os.listdir(saved_pictures):\n",
    " person_path = os.path.join(saved_pictures, person_folder)\n",
    " for file in os.listdir(person_path):\n",
    "  if file.endswith(\".jpeg\"):\n",
    "   img = cv2.imread(os.path.join(person_path, file))\n",
    "   cropped = mtcnn(img)\n",
    "   if cropped is not None:\n",
    "    all_people_faces[person_folder] = encode(cropped)[0, :]\n",
    "    known_distances.append(encode(cropped)[0, :])\n",
    "\n",
    "# Calculate a dynamic threshold based on the distances in the known dataset\n",
    "known_distances = torch.stack(known_distances)\n",
    "dynamic_threshold = np.percentile([dist.norm().item() for dist in known_distances], 90) # Adjust percentile as needed\n",
    "     \n",
    "     \n",
    "# Initialize lists for storing predicted and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "confidence_scores = [] # List to store confidence scores\n",
    "\n",
    "# Initialize a counter for correct predictions\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "### initialize webcam and capture loop\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    " # Capture frame-by-frame\n",
    " ret, frame = cap.read()\n",
    "    \n",
    " # Skip detection every N frames (reduce processing)\n",
    " detection_counter += 1\n",
    " if detection_counter < detection_frequency:\n",
    "    continue\n",
    " detection_counter = 0\n",
    "\n",
    " # Detect faces in the frame\n",
    " batch_boxes, cropped_images = mtcnn.detect_box(frame)\n",
    "\n",
    " # Recognition for each detected face\n",
    " if cropped_images is not None:\n",
    "  for box, cropped in zip(batch_boxes, cropped_images):\n",
    "   x, y, x2, y2 = [int(x) for x in box]\n",
    "   img_embedding = encode(cropped.unsqueeze(0))\n",
    "\n",
    "   min_distance = float(\"inf\")\n",
    "   min_key = \"Undetected\"\n",
    "   for k, v in all_people_faces.items():\n",
    "    distance = (v - img_embedding).norm().item()\n",
    "    if distance < min_distance:\n",
    "     min_distance = distance\n",
    "     min_key = k\n",
    "       \n",
    "   total_predictions += 1\n",
    "\n",
    "    # Use dynamic threshold\n",
    "   if min_distance < dynamic_threshold:\n",
    "      correct_predictions += 1  \n",
    "       \n",
    "   # Append predicted and true labels\n",
    "   predicted_labels.append(min_key)\n",
    "   true_labels.append(k) # Assuming file name is the actual label\n",
    "   confidence_scores.append(1 - min_distance) # Confidence score (1 - distance)\n",
    "\n",
    "   # Draw bounding box and label\n",
    "   if min_distance < 0.7:\n",
    "    cv2.rectangle(frame, (x, y), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(frame, min_key, (x + 5, y + 10), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 255, 0), 1)\n",
    "   else:\n",
    "    cv2.rectangle(frame, (x, y), (x2, y2), (0, 0, 255), 2)\n",
    "    cv2.putText(frame, \"Undetected\", (x + 5, y + 10), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    " # Display the resulting frame\n",
    " cv2.imshow(\"Real-time face recognition\", frame)\n",
    "\n",
    " # Quit loop on ' ' key press\n",
    " if cv2.waitKey(1) == ord(' '):\n",
    "  break \n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "# Calculate mean average precision (mAP)\n",
    "mAP = average_precision_score((np.array(true_labels) != \"Undetected\").astype(int), confidence_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}% (Dynamic Threshold: {dynamic_threshold:.2f})\")\n",
    "print(f\"Mean Average Precision (mAP): {mAP:.2f}\")\n",
    "   \n",
    "# Release capture device\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126d15f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
